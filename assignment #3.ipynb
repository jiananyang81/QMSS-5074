{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG+f765rt0cG9M/dhTB055",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiananyang81/QMSS-5074/blob/main/assignment%20%233.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "link to Github repo:https://github.com/jiananyang81/QMSS-5074"
      ],
      "metadata": {
        "id": "R0I02ryTK-RR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part I: Use the IMDB LSTM notebook from last week's class to access data and to create your own training and test dataset."
      ],
      "metadata": {
        "id": "DFOKsOkxqMw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discuss the dataset in general terms and describe why building a predictive model using this data might be practically useful.  Who could benefit from a model like this? Explain."
      ],
      "metadata": {
        "id": "-02fSe6gqZwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB dataset having 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data. They provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms is possible. This model could be advantageous for companies. By examining the sentiment of movie reviews, companies can pinpoint areas that need improvement and make informed decisions regarding their marketing strategies. Additionally, customers can also benefit from this model. They can use the predictions to determine which movies to watch and which ones to avoid."
      ],
      "metadata": {
        "id": "EbV4-8sE0cTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run at least three prediction models to try to predict the IMDB sentiment dataset well.\n",
        "Use an Embedding layer and LSTM layers in at least one model\n",
        "Use an Embedding layer and Conv1d layers in at least one model\n",
        "Use transfer learning with glove embeddings for at least one of these models\n",
        "Discuss which models performed better and point out relevant hyper-parameter values for successful models\n"
      ],
      "metadata": {
        "id": "tOsIPS66qgha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get raw imdb dataset\n",
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7CV2lfAvzjU",
        "outputId": "29c11b02-8ed0-489c-8c16-a654ae51796a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-16 18:37:03--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  17.5MB/s    in 7.5s    \n",
            "\n",
            "2023-12-16 18:37:10 (10.8 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Untar it to a new folder\n",
        "! tar xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "r61NVIjFqqdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build corpus of docs and labels\n",
        "import os\n",
        "\n",
        "imdb_dir = 'aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "metadata": {
        "id": "Pu2I9Envqm6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKxBRAbazSt1",
        "outputId": "3f69348c-9500-4a7c-a255-0706bd822af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0])\n",
        "print(labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMdLowIvqh_M",
        "outputId": "c9a732a7-3d03-4bf6-ff34-14336ece06d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This movie tries to hard to be something that it's not....a good movie. It wants you to be fooled from begining to end,But fails.From when it starts to get interesting it falls apart and you're just hoping the ending gives you some clue of just what is going on but it didn't.<br /><br />\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data into one hot vectors\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # We will cut reviews after 100 words in sequence\n",
        "training_samples = 10000  # We will be training on 10000 samples\n",
        "validation_samples = 10000  # We will be validating on 10000 samples\n",
        "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts) # converts words in each text to each word's numeric index in tokenizer dictionary.\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Split the data into a training set and a validation set\n",
        "# But first, shuffle the data, since we started from data\n",
        "# where sample are ordered (all negative first, then all positive).\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples] #100 words\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDCjEmdyqaq4",
        "outputId": "fa1e16c0-4863-4339-e910-da73ce322f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 88582 unique tokens.\n",
            "Shape of data tensor: (25000, 100)\n",
            "Shape of label tensor: (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text example\n",
        "print(texts[0])\n",
        "\n",
        "#Text transformed to sequence using tokenizer\n",
        "print(sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2tSV16XqUXG",
        "outputId": "314b26fb-271b-4013-96b9-0ac6b3e20af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This movie tries to hard to be something that it's not....a good movie. It wants you to be fooled from begining to end,But fails.From when it starts to get interesting it falls apart and you're just hoping the ending gives you some clue of just what is going on but it didn't.<br /><br />\n",
            "[11, 17, 494, 5, 251, 5, 27, 139, 12, 42, 21, 3, 49, 17, 9, 490, 22, 5, 27, 4449, 36, 5, 127, 18, 993, 36, 51, 9, 514, 5, 76, 218, 9, 731, 968, 2, 332, 40, 1379, 1, 274, 405, 22, 46, 2297, 4, 40, 48, 6, 167, 20, 18, 9, 158, 7, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sequences preprocessed with tokenizer with zeroes added whenver text isn't 100 words long.\n",
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TPCfDKU1O60",
        "outputId": "1b738b7e-bb41-4d53-e4b1-be343f50059d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  71,  230,  184,   87,   28,  248,   24,  319, 1065,    5, 8090,\n",
              "          1,  516,  234,   10,   89,  178,    5, 2368,   21,   12,   47,\n",
              "         13,  230,    5, 2368,   10, 3422,  384,  126, 1542,  819,   43,\n",
              "          1, 9019,    2,   10,  358,   12,  678, 9060, 3888,   12,  791,\n",
              "          7,    7,   10,   66,    5,  545,    5,  785, 4160,  140,   11,\n",
              "          3,  783, 8903,    4,    3,   19,    2,   10,   59,   37,    5,\n",
              "        132,   12,   44,   22,   23,    5,    1,   17, 1129,    2,   23,\n",
              "        533,   41,  109, 9545,  588,   89,   42,    3,  434,    4,    1,\n",
              "         19,    9,   13,   20,    7,    7,   92,  171,   10,   97,   27,\n",
              "        352], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use transfer learning with glove embeddings for at least one of these models"
      ],
      "metadata": {
        "id": "i_l7dZ8CBMk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start with a model that ignores the sequential steps that make up each observation\n",
        "from tensorflow.keras.layers import Dense, Embedding,Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "# Specify the size of your vocabulary (i.e.-10,000 terms)\n",
        "# Specify the number of features you want to extract via fitting weights to your embedding matrix.\n",
        "# We also specify the maximum input length to our Embedding layer\n",
        "# so we can later flatten the embedded inputs\n",
        "model.add(Embedding(10000, 16, input_length=maxlen))\n",
        "# After the Embedding layer,\n",
        "# our activations have shape `(samples, maxlen, 8)`.\n",
        "\n",
        "# We flatten the 3D tensor of embeddings\n",
        "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
        "model.add(Flatten())\n",
        "\n",
        "# We add the classifier on top\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1X3GzF--5fK",
        "outputId": "4da8ba5a-2cd2-4520-b080-4be8d8c2d741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 100, 16)           160000    \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 1601      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 161601 (631.25 KB)\n",
            "Trainable params: 161601 (631.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.6937 - acc: 0.4970 - val_loss: 0.6926 - val_acc: 0.5190\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.6703 - acc: 0.7315 - val_loss: 0.6925 - val_acc: 0.5230\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.6221 - acc: 0.8404 - val_loss: 0.6951 - val_acc: 0.5150\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.5439 - acc: 0.8950 - val_loss: 0.7035 - val_acc: 0.5105\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4474 - acc: 0.9326 - val_loss: 0.7196 - val_acc: 0.5100\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3495 - acc: 0.9559 - val_loss: 0.7437 - val_acc: 0.5115\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.2613 - acc: 0.9735 - val_loss: 0.7760 - val_acc: 0.5165\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.1885 - acc: 0.9850 - val_loss: 0.8161 - val_acc: 0.5255\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.1315 - acc: 0.9919 - val_loss: 0.8600 - val_acc: 0.5190\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0894 - acc: 0.9952 - val_loss: 0.9132 - val_acc: 0.5120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What does the output of the embedding layer look like?  (It returns a sequentially ordered transformation of numerically indexed input word data)\n",
        "# Credit: Code adjusted from ML Mastery\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(10000, 8, input_length=100))\n",
        "# The model will take as input an integer matrix of size (batch,\n",
        "# input_length).\n",
        "# Now model.output_shape is (None, 100, 8), where `None` is the batch\n",
        "# dimension.\n",
        "input_array = np.random.randint(1000, size=(1, 100))\n",
        "model.compile('rmsprop', 'mse')\n",
        "output_array = model.predict(input_array)\n",
        "print(output_array.shape)\n",
        "print(output_array[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_bUt27d_VCV",
        "outputId": "d6d84c8a-0bdc-4f0d-c486-4df639415951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 164ms/step\n",
            "(1, 100, 8)\n",
            "[[-0.04039253 -0.04411009 -0.03234978  0.01725081  0.0454383   0.02619984\n",
            "  -0.03623825  0.00021284]\n",
            " [ 0.00197277  0.04725694 -0.02286728  0.00680789  0.0391122   0.03458827\n",
            "  -0.03983982 -0.01803257]\n",
            " [ 0.01051817 -0.0213825   0.03032181 -0.00964444  0.0290267  -0.03048908\n",
            "   0.01417166 -0.037655  ]\n",
            " [-0.03853192  0.03989482 -0.03746786 -0.0452791  -0.02051405 -0.0036217\n",
            "  -0.0175675   0.03449437]\n",
            " [-0.02627106 -0.03013691  0.00368441 -0.0293501  -0.00521041  0.02580566\n",
            "  -0.03471079 -0.04385408]\n",
            " [ 0.00995962  0.00254407  0.03331595 -0.02559943  0.02167854 -0.04757075\n",
            "   0.04240945  0.02990988]\n",
            " [ 0.0421157   0.01651325  0.01130859 -0.02158054  0.01287824 -0.03106356\n",
            "  -0.03269391 -0.01969727]\n",
            " [-0.0143639   0.01808712  0.00882574 -0.03894784 -0.02497778  0.00896768\n",
            "   0.01192253 -0.02855209]\n",
            " [-0.00175601  0.04915081  0.01583591 -0.00799948 -0.01359546 -0.00654449\n",
            "  -0.04536191  0.01702083]\n",
            " [-0.00455693  0.02848324 -0.02561255  0.02490567  0.0220615   0.01689607\n",
            "  -0.00701576  0.04045259]\n",
            " [ 0.00381298 -0.00383425  0.04417506  0.03509713 -0.02634158 -0.00664761\n",
            "   0.0022018  -0.01240339]\n",
            " [ 0.02224259 -0.0454393   0.00316129  0.01777842  0.02576867  0.02666305\n",
            "  -0.01253993  0.02312138]\n",
            " [ 0.01436077  0.03633349 -0.01784755  0.02000591 -0.03724011 -0.04917498\n",
            "   0.0012779  -0.0423327 ]\n",
            " [-0.02214709 -0.03124008  0.04081848 -0.03056593  0.04363021  0.04535774\n",
            "   0.01091326 -0.00851904]\n",
            " [-0.02607867 -0.00634357 -0.00957926  0.04970232 -0.03204454 -0.01070329\n",
            "  -0.03860528 -0.0382087 ]\n",
            " [ 0.0005641  -0.04536822 -0.01072339  0.04923576 -0.0099705   0.03259107\n",
            "   0.01768762 -0.00044961]\n",
            " [-0.03514707 -0.00544553 -0.01412945 -0.00098809  0.02214993 -0.03239067\n",
            "  -0.01958892  0.03967304]\n",
            " [ 0.02719447 -0.00211756  0.03366505 -0.01830374  0.01342002  0.01449159\n",
            "  -0.01188968 -0.01753946]\n",
            " [-0.04350053  0.02126275 -0.00265414 -0.00853052 -0.00841569  0.04423441\n",
            "  -0.02676784 -0.04202888]\n",
            " [-0.02278712  0.04866183  0.04950464 -0.03390521  0.02872558  0.02724062\n",
            "   0.02734664 -0.01975191]\n",
            " [ 0.0031916   0.00872971  0.01520416 -0.00390409  0.03350255 -0.02445487\n",
            "   0.02181074 -0.02115309]\n",
            " [ 0.01227559 -0.02566441 -0.02267195 -0.03077568  0.03663576  0.04721074\n",
            "  -0.02664228 -0.04941429]\n",
            " [ 0.01275047 -0.03777904 -0.00507014  0.01188946  0.00231659  0.01422273\n",
            "   0.04162696  0.00826607]\n",
            " [ 0.00640271  0.03985191  0.04094974  0.04028038 -0.01767679 -0.04732008\n",
            "   0.03009147 -0.03179138]\n",
            " [ 0.02997475  0.02608516  0.03035114  0.04402648  0.015573   -0.0419355\n",
            "   0.03285171  0.01827632]\n",
            " [-0.00638985  0.00190482 -0.03143238 -0.04760599  0.04022757  0.00133222\n",
            "  -0.04256969  0.03262773]\n",
            " [ 0.01179422 -0.03232185 -0.0433466  -0.00632502 -0.0263489   0.01299217\n",
            "   0.04579905  0.02664253]\n",
            " [-0.01316432 -0.01776931 -0.0479221  -0.02990611 -0.03764417 -0.02474898\n",
            "   0.04422834 -0.01276542]\n",
            " [ 0.01200612 -0.04393352  0.01366583  0.0147248   0.01861837  0.00170217\n",
            "   0.00165201 -0.02356048]\n",
            " [ 0.02893198 -0.04125005 -0.00942902  0.00632942 -0.03095547  0.01573627\n",
            "  -0.01126935  0.00994305]\n",
            " [-0.04489751  0.04635687 -0.02540814 -0.01619742  0.04151822  0.02839793\n",
            "   0.01102837 -0.0247297 ]\n",
            " [ 0.02801699  0.01862836 -0.01317523 -0.04017039  0.02944991 -0.00765005\n",
            "  -0.01651294  0.01578524]\n",
            " [ 0.03870994 -0.02064017  0.04837009  0.0285337   0.04234043 -0.00360193\n",
            "  -0.03441123  0.0081949 ]\n",
            " [-0.02498053  0.00632475  0.02660114  0.02464243  0.01300526 -0.03484578\n",
            "  -0.00043933  0.01287022]\n",
            " [ 0.03535173 -0.0227391  -0.01228601  0.00321134  0.03893651 -0.04743824\n",
            "  -0.03066146 -0.03855506]\n",
            " [-0.01751832 -0.03101695 -0.02137506 -0.02166355  0.00667519  0.02315671\n",
            "   0.03610766  0.02925536]\n",
            " [ 0.00549608 -0.03545843 -0.00414188 -0.01910296 -0.00527222  0.01578588\n",
            "  -0.04716721 -0.03967996]\n",
            " [-0.0381768  -0.01225124  0.03698087 -0.00526464 -0.01356829 -0.00541313\n",
            "  -0.02514165  0.04589653]\n",
            " [-0.01316432 -0.01776931 -0.0479221  -0.02990611 -0.03764417 -0.02474898\n",
            "   0.04422834 -0.01276542]\n",
            " [ 0.02392613 -0.00090856 -0.03200392  0.01423449  0.01456965 -0.02972604\n",
            "  -0.01722474  0.0478896 ]\n",
            " [-0.00752858  0.04676825  0.0018707   0.01454934 -0.00361074  0.01593665\n",
            "   0.02819716 -0.03542533]\n",
            " [-0.02845681 -0.02180641  0.04340747 -0.04487453  0.00222514  0.01786015\n",
            "   0.02730716 -0.00651473]\n",
            " [-0.02860544 -0.0008158  -0.00491551 -0.01879157 -0.02793973  0.04075244\n",
            "   0.04193466 -0.04308945]\n",
            " [ 0.04826144  0.02544436  0.01397343 -0.04620665 -0.0079359   0.01049043\n",
            "   0.01605899 -0.02633307]\n",
            " [-0.04072697 -0.01712472 -0.03578951 -0.02949449 -0.04504186 -0.03230548\n",
            "   0.03914103  0.00937923]\n",
            " [-0.00894435  0.02354348 -0.00818541 -0.01467434 -0.00804625  0.0183517\n",
            "  -0.00934158  0.01238082]\n",
            " [-0.03509993  0.00884938 -0.04431171  0.02878052  0.03026315 -0.02055568\n",
            "  -0.0074762  -0.01373593]\n",
            " [-0.0162873   0.0344231  -0.01754848  0.01711926  0.00157376 -0.04566878\n",
            "   0.00892099  0.03599173]\n",
            " [-0.0218868   0.02549184  0.03807597  0.01141061 -0.01836972 -0.03992261\n",
            "   0.03700621  0.00181894]\n",
            " [ 0.0366883   0.01282339  0.03365523  0.02803227  0.03457521 -0.01774309\n",
            "  -0.02151219  0.01346983]\n",
            " [-0.04602743  0.04214653  0.04631699 -0.04382015 -0.00919764 -0.01861697\n",
            "  -0.01964958  0.03439647]\n",
            " [ 0.03257241  0.0264569   0.03416456  0.04699228 -0.01483033  0.03935932\n",
            "   0.04960215 -0.03042269]\n",
            " [-0.01447847 -0.03488283 -0.03956685 -0.04091798 -0.02195182 -0.02701714\n",
            "  -0.03408094  0.0088256 ]\n",
            " [ 0.0281332   0.03124267 -0.04749925 -0.03122902 -0.02965114 -0.02000625\n",
            "   0.009345    0.04658324]\n",
            " [-0.00800221 -0.00107046  0.02087325  0.01463776  0.0344859   0.00173463\n",
            "   0.01130772 -0.04631984]\n",
            " [ 0.00866843  0.04898094  0.03027229  0.02389571  0.01458726 -0.02811727\n",
            "  -0.03401494  0.0204093 ]\n",
            " [ 0.02357109 -0.0042824   0.01870694  0.04376623  0.0447317  -0.03895068\n",
            "  -0.04103585  0.01800487]\n",
            " [ 0.0183405   0.00617828 -0.00372456 -0.00225415 -0.00476838 -0.04551147\n",
            "   0.03707803 -0.02274149]\n",
            " [ 0.04671696 -0.01776642  0.03198859  0.03986783 -0.00922401 -0.03354877\n",
            "  -0.04018055 -0.03629125]\n",
            " [-0.04917521  0.04066269  0.02745267  0.04322881 -0.03800094  0.02247695\n",
            "  -0.03649367 -0.02255817]\n",
            " [-0.02406744 -0.03847067 -0.02892375 -0.04229951 -0.02478886 -0.00443033\n",
            "  -0.00489464 -0.02346634]\n",
            " [-0.01275065 -0.04413695 -0.02096833  0.04806307  0.01678488  0.03728769\n",
            "   0.03243387 -0.01239536]\n",
            " [ 0.01300656 -0.02106013  0.00982542 -0.0042591  -0.02184415  0.00571773\n",
            "  -0.03744344 -0.0004895 ]\n",
            " [-0.01836603  0.03113944 -0.01836014 -0.03786516  0.03384738  0.02655805\n",
            "  -0.00489687  0.03340245]\n",
            " [-0.0100976  -0.04622721 -0.0103006   0.01278264  0.00578719  0.03025163\n",
            "   0.04541149  0.00694661]\n",
            " [ 0.02473963 -0.02806935  0.00339044 -0.01555463  0.03073758  0.02557168\n",
            "  -0.00192264 -0.04762479]\n",
            " [ 0.01101906  0.00249358 -0.02693894 -0.00940226 -0.04361124  0.02201823\n",
            "   0.02285481  0.00492909]\n",
            " [ 0.02860289 -0.01018608  0.01244266  0.00933294  0.03133941  0.02606371\n",
            "   0.02574079  0.02492933]\n",
            " [ 0.01821002 -0.03171398  0.04938563  0.04018245  0.00360634 -0.03548069\n",
            "   0.00546371 -0.03749371]\n",
            " [-0.04721812 -0.0319108  -0.03333741  0.02152694  0.04893317  0.0035617\n",
            "  -0.04927747 -0.0383487 ]\n",
            " [ 0.03360543 -0.00245043 -0.00482321  0.04254905 -0.04821623 -0.02972065\n",
            "   0.04736559  0.01414623]\n",
            " [-0.03270396  0.00589513  0.01223085 -0.03822309 -0.03211784  0.00440269\n",
            "   0.00275278  0.01176082]\n",
            " [-0.03137352  0.03202732 -0.04634626 -0.04755609  0.01470485  0.04696617\n",
            "  -0.04344974 -0.0067881 ]\n",
            " [-0.04656407 -0.04634875 -0.03424349  0.00170617  0.00225983 -0.00106402\n",
            "   0.03189399 -0.01200525]\n",
            " [ 0.00667133 -0.01547811  0.00246397 -0.00747689 -0.03355109 -0.02529475\n",
            "  -0.04867045 -0.02738713]\n",
            " [-0.02069005 -0.02374996  0.03080158  0.046356    0.04927829  0.04807768\n",
            "   0.00209286 -0.02134534]\n",
            " [-0.01699525 -0.01565826 -0.02773875 -0.04374206 -0.00552559 -0.00122932\n",
            "  -0.00274473  0.00888304]\n",
            " [-0.01966465  0.04570705 -0.0491913  -0.00378161 -0.03542332  0.0100211\n",
            "  -0.01992851  0.04737787]\n",
            " [-0.04756038  0.0050656   0.02985566  0.03196832 -0.0136286  -0.02712786\n",
            "  -0.01636372 -0.01066812]\n",
            " [ 0.01023632  0.04594345  0.0425519  -0.0110307   0.04556278 -0.00982534\n",
            "   0.04278329  0.02880209]\n",
            " [-0.03844647 -0.03212573 -0.02472632  0.03466138 -0.00500698 -0.04477457\n",
            "   0.03439137  0.00575794]\n",
            " [-0.04008939  0.04514555 -0.04083193  0.01054019  0.04970426 -0.01367217\n",
            "   0.04640928  0.01215982]\n",
            " [-0.04731999  0.02375582  0.03929349  0.00372261 -0.01088988 -0.01597176\n",
            "   0.01059071  0.02337481]\n",
            " [-0.00822825  0.04431908  0.03373158 -0.02180326  0.00314454  0.01036442\n",
            "   0.02334261  0.00444411]\n",
            " [-0.03780176  0.02979505 -0.00755813  0.00951342 -0.00856053  0.01997638\n",
            "  -0.04191271 -0.01912087]\n",
            " [ 0.01536392 -0.03634785 -0.01329244 -0.03415929 -0.03132717  0.04437921\n",
            "  -0.03757982  0.02160903]\n",
            " [ 0.00865817 -0.0206882   0.03294231 -0.01621222  0.01515709 -0.03510188\n",
            "  -0.0357535   0.02472831]\n",
            " [-0.04520231  0.01718278 -0.01360454 -0.04854463  0.00836749  0.02594805\n",
            "   0.01568544  0.01421552]\n",
            " [ 0.0218859  -0.02211022  0.0434613  -0.03961495  0.0401835   0.02638086\n",
            "  -0.01363181 -0.00310551]\n",
            " [ 0.0066787   0.02455189  0.03442247  0.02670327  0.04762277  0.04413514\n",
            "  -0.02534816  0.04433567]\n",
            " [ 0.03257241  0.0264569   0.03416456  0.04699228 -0.01483033  0.03935932\n",
            "   0.04960215 -0.03042269]\n",
            " [ 0.0492824  -0.00068219 -0.00434893 -0.02200263 -0.04940661 -0.02123902\n",
            "   0.01497009 -0.00292208]\n",
            " [-0.01465676  0.04589722 -0.03168359  0.02350085 -0.03604916  0.02476874\n",
            "  -0.04442682 -0.0349646 ]\n",
            " [-0.00805587  0.03318464  0.04458391  0.04820755 -0.01495502  0.00320505\n",
            "   0.01884519 -0.00371612]\n",
            " [-0.03059242  0.02551229 -0.04109784 -0.02509677 -0.00226742  0.01019282\n",
            "   0.01595792  0.00712765]\n",
            " [ 0.03093393  0.04018091  0.00327937 -0.02294562  0.03821291  0.04095565\n",
            "   0.01648673  0.02666444]\n",
            " [-0.0388382  -0.00507202  0.02061481 -0.00119738 -0.00392681  0.04394777\n",
            "  -0.01605273  0.02873283]\n",
            " [-0.02036562 -0.01861997 -0.03914221 -0.03205067  0.00682287  0.0034253\n",
            "  -0.03550308  0.0073828 ]\n",
            " [-0.00614246  0.0311785  -0.03803431 -0.02093225  0.01923262 -0.0161707\n",
            "  -0.00755208  0.04696761]\n",
            " [-0.00312059  0.04970137  0.03758698 -0.01413693 -0.0284698   0.02033261\n",
            "  -0.00742108 -0.02368256]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_y8MWvE_oWS",
        "outputId": "4b693203-d89c-4d97-e75f-878d84d4d522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-16 18:58:30--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
            "--2023-12-16 18:58:30--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2023-12-16 18:58:31--  https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.14MB/s    in 2m 44s  \n",
            "\n",
            "2023-12-16 19:01:15 (5.01 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBO8qEA-AbIK",
        "outputId": "243fe877-c9e4-4865-9706-9527017d5cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embedding data for 100 feature embedding matrix\n",
        "glove_dir = os.getcwd()\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfCxFt0aAlE9",
        "outputId": "f30abf67-2bcb-4175-c904-bbfc3fe8d740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400001 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build embedding matrix\n",
        "embedding_dim = 100 # change if you use txt files using larger number of features\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "1SXVocuVAq-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up same model architecture as before and then import Glove weights to Embedding layer:\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oBsTdeDAtLH",
        "outputId": "f4c425a2-6418-4c6e-ddb9-a4e016551f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1320065 (5.04 MB)\n",
            "Trainable params: 1320065 (5.04 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add weights in same manner as transfer learning and turn of trainable option before fitting model to freeze weights.\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')\n",
        "\n",
        "\n",
        "# Training data small to speed up training. Increase for better fit.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqVtiNkqAxfi",
        "outputId": "300ea766-3649-48db-c0b2-99b89075ce05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 4s 10ms/step - loss: 0.7288 - acc: 0.5072 - val_loss: 0.6933 - val_acc: 0.4992\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 0.6984 - acc: 0.5097 - val_loss: 0.6933 - val_acc: 0.5000\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 0.6928 - acc: 0.5183 - val_loss: 0.6941 - val_acc: 0.5006\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.6822 - acc: 0.5457 - val_loss: 0.7009 - val_acc: 0.5044\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.6590 - acc: 0.5922 - val_loss: 0.7092 - val_acc: 0.5019\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.6079 - acc: 0.6466 - val_loss: 0.7644 - val_acc: 0.4955\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.5419 - acc: 0.7124 - val_loss: 0.8019 - val_acc: 0.5011\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.4568 - acc: 0.7764 - val_loss: 1.2611 - val_acc: 0.4946\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3695 - acc: 0.8377 - val_loss: 1.0121 - val_acc: 0.4944\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2910 - acc: 0.8719 - val_loss: 1.1902 - val_acc: 0.5013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test set (need to preprocess test data to same structure first)\n",
        "\n",
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "#using tokenizer object we fit to test data above\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "metadata": {
        "id": "rIGe3gxIA59i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn-Wv2jvA-rT",
        "outputId": "70eeebf5-a89c-4864-f650-dad9ce50fa1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 3ms/step - loss: 1.2172 - acc: 0.4923\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2171597480773926, 0.4923200011253357]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use an Embedding layer and LSTM layers in at least one model"
      ],
      "metadata": {
        "id": "Q0mvTeq-AkvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Embedding(10000, 32, input_length=maxlen))\n",
        "model2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model2.fit(x_train, y_train,\n",
        "                    epochs=1,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7unSA2tkCD_u",
        "outputId": "d9f911e1-015a-4854-8298-680ea845f4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250/250 [==============================] - 65s 250ms/step - loss: 0.6936 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model2.evaluate(x_test, y_test)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9zZX8fqDolI",
        "outputId": "438d4c1a-9667-4c91-de9e-23eadfa0b38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 32s 41ms/step - loss: 0.6932 - acc: 0.4963\n",
            "Test score: 0.6932473182678223\n",
            "Test accuracy: 0.4963200092315674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use an Embedding layer and Conv1d layers in at least one model"
      ],
      "metadata": {
        "id": "Ofc4syvmEEAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use 1D Conv layer rather than RNN or LSTM or GRU to fit model\n",
        "# Why? Much lighter model to fit. Here we are training on the full dataset.  If you try\n",
        "# to build a model using LSTM code after running this one it will be much slower.\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM,Embedding\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(layers.Embedding(10000, 8, input_length=maxlen))\n",
        "model3.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model3.add(layers.MaxPooling1D(5)) #\n",
        "model3.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model3.add(layers.GlobalMaxPooling1D())\n",
        "model3.add(layers.Dense(1))\n",
        "\n",
        "model3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f7bJseNEEX5",
        "outputId": "667c0f41-5ed0-42e9-8ad4-e927f44e9617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_11 (Embedding)    (None, 100, 8)            80000     \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 94, 32)            1824      \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 18, 32)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 12, 32)            7200      \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 32)                0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 89057 (347.88 KB)\n",
            "Trainable params: 89057 (347.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(optimizer=RMSprop(lr=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model3.fit(x_train, y_train,\n",
        "                    epochs=1,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqYBsfOgEYsq",
        "outputId": "03792b11-ebf5-4e68-fcdf-0bfa71f35105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 3s 24ms/step - loss: 7.7414 - acc: 0.4981 - val_loss: 7.8667 - val_acc: 0.4900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model3.evaluate(x_test, y_test)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkeqgqbHEsWm",
        "outputId": "d978632c-966c-4baf-a5d8-275eef50942e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 3ms/step - loss: 7.7125 - acc: 0.5000\n",
            "Test score: 7.712466239929199\n",
            "Test accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding layer and Conv1d layers perform better."
      ],
      "metadata": {
        "id": "fQjHAtjgFQnB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tyYTSXF4FFzz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}